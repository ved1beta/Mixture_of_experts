# Mixture of Experts (MoE) Implementation

This project implements a Mixture of Experts (MoE) model with a focus on efficient routing and expert utilization. The implementation includes:

1. A flexible MoE router that can handle different numbers of experts and routing parameters
2. Expert layers implemented as feed-forward networks
3. A complete MoE layer that combines routing and expert computation
4. A transformer block that uses MoE for the feed-forward layer
5. Comprehensive benchmarking utilities

## Project Structure

```
src/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ router.py      # MoE router implementation
â”‚   â”œâ”€â”€ experts.py     # Expert layer implementation
â”‚   â”œâ”€â”€ moe.py         # Main MoE layer
â”‚   â””â”€â”€ transformer.py # Transformer block with MoE
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ benchmarking.py # Benchmarking utilities
â”œâ”€â”€ triton/
â”‚   â”œâ”€â”€ README.md      # Triton implementation documentation
â”‚   â””â”€â”€ router/        # Triton router implementation
â”‚       â”œâ”€â”€ top_k_router.py # Triton MoE router
â”‚       â””â”€â”€ test_router.py  # Testing utilities
â”œâ”€â”€ benchmark.py       # Main benchmark script
â”œâ”€â”€ profile_moe.py     # Profiling script for memory and performance analysis
â””â”€â”€ profiling.py       # General profiling utilities
```

## Features

- Efficient routing algorithm with support for:
  - Top-k expert selection
  - Capacity factor for load balancing
  - Optional router jitter for improved training
- Expert layers with configurable dimensions
- Comprehensive benchmarking suite that measures:
  - Router performance
  - Full MoE layer performance
  - Expert utilization
  - Impact of different parameters (k, capacity factor)
- Advanced profiling tools for:
  - Memory usage analysis
  - Performance bottleneck identification
  - Detailed timing of different components
- Optimized Triton implementation (in progress):
  - Memory-efficient routing
  - Optimized top-k selection
  - Reduced thread divergence

## Installation

1. Create a virtual environment:
```bash
python -m venv venv
source venv/bin/activate  # On Linux/Mac
# or
.\venv\Scripts\activate  # On Windows
```

2. Install dependencies:
```bash
pip install -r requirements.txt
```

## Usage

### Running Benchmarks

Run the benchmark suite:
```bash
python src/benchmark.py
```

This will:
1. Run a comprehensive benchmark suite with different configurations
2. Analyze expert utilization and generate plots
3. Profile performance with different parameters

### Profiling the Implementation

Run the profiling script:
```bash
python src/profile_moe.py
```

This will:
1. Profile the MoE router, MoE layer, and transformer block
2. Generate detailed reports on memory usage and performance
3. Identify key bottlenecks for optimization
4. Update the baseline profiling documentation

### Testing the Triton Implementation

Run the Triton router tests:
```bash
python -m src.triton.router.test_router
```

This will:
1. Test the correctness of the Triton implementation against PyTorch
2. Benchmark the performance of both implementations
3. Generate a report with the results

## Results

The benchmarking and profiling results will be saved as:
- Expert utilization plots: `expert_utilization_k{k}_cf{capacity_factor}.png`
- Profiling reports: `docs/router_profiling_report.md`, `docs/moe_layer_profiling_report.md`, etc.
- Consolidated profiling report: `docs/consolidated_profiling_report.md`
- Baseline profiling documentation: `docs/baseline_profiling.md`
- Triton benchmark results: `triton_router_benchmark_results.txt`

## Project Roadmap

1. âœ… Baseline PyTorch Implementation
   - âœ… Router implementation
   - âœ… Expert implementation
   - âœ… MoE layer implementation
   - âœ… Transformer integration

2. âœ… Evaluation Framework
   - âœ… Benchmarking utilities
   - âœ… Expert utilization analysis
   - âœ… Performance profiling

3. ğŸ”„ Triton Implementation (In Progress)
   - âœ… Interface definition
   - âœ… Basic structure
   - âŒ Core kernels implementation
   - âŒ Optimization and tuning

4. âŒ Advanced Optimizations (Planned)
   - âŒ Memory optimization
   - âŒ Quantization support
   - âŒ Multi-GPU support

## Contributing

Feel free to submit issues and enhancement requests!
